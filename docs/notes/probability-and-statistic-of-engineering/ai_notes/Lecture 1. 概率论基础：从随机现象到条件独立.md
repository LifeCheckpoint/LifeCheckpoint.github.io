---
title: 01. 概率论基础：从随机现象到条件独立
authors: [Life_Checkpoint]
tags: [note]
sidebar_position: 1
---


## 1.1 基础概念

### 1.1.1 随机试验与事件

概率论研究的核心是**随机现象**，其在数学上的形式化描述依赖于以下基本概念：

-   **随机试验 (Random Experiment)**：满足以下三个条件的试验：
    1.  可在相同条件下重复进行。
    2.  所有可能的结果都明确可知。
    3.  每次试验前无法确定哪个结果会发生。

-   **样本空间 (Sample Space, $\Omega$)**：一个随机试验所有可能的基本结果（**样本点 (Sample Point, $\omega$)**）的集合。

-   **随机事件 (Random Event, $A, B, C, ...$)**：样本空间 $\Omega$ 的一个子集。当试验结果 $\omega$ 属于事件 $A$（即 $\omega \in A$）时，称事件 $A$ 发生。

### 1.1.2 事件的关系与运算

事件作为集合，其关系与运算遵循集合论的法则，这为我们分析复杂事件提供了清晰的框架。

| 关系/运算 | 记法 | 含义 |
| :--- | :--- | :--- |
| **包含 (Inclusion)** | $A \subset B$ | $A$ 发生必然导致 $B$ 发生 |
| **和/并 (Sum/Union)** | $A \cup B$ 或 $A+B$ | $A$ 与 $B$ 至少有一个发生 |
| **积/交 (Product/Intersection)** | $A \cap B$ 或 $AB$ | $A$ 与 $B$ 同时发生 |
| **差 (Difference)** | $A - B$ | $A$ 发生但 $B$ 不发生 |
| **<u>互斥 (Mutually Exclusive)</u>** | $A \cap B = \emptyset$ | $A$ 与 $B$ 不能同时发生 |
| **对立 (Complement)** | $B = \bar{A}$ | $A \cap B = \emptyset$ 且 $A \cup B = \Omega$，即 $A$ 与 $B$ 必有一个发生且仅发生一个 |

**事件的运算法则**：
-   交换律 (Communicative laws): $A \cup B = B \cup A$, $A \cap B = B \cap A$
-   结合律 (Associative laws): $(A \cup B) \cup C = A \cup (B \cup C)$, $(A \cap B) \cap C = A \cap (B \cap C)$
-   分配律 (Distributive law): $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
-   **德·摩根律 (De Morgan's laws)**:
    $$
    \overline{\bigcup_{i=1}^n A_i} = \bigcap_{i=1}^n \overline{A_i}, \quad \overline{\bigcap_{i=1}^n A_i} = \bigcup_{i=1}^n \overline{A_i}
    $$

### 1.1.3 概率的公理化定义

概率是对随机事件发生可能性大小的度量。其严格的数学定义由以下三条公理给出：

设 $P(\cdot)$ 是定义在样本空间 $\Omega$ 子集上的一个实值函数，若其满足：
1.  **非负性 (Non-negativity)**：对于任意事件 $A$，有 $P(A) \ge 0$。
2.  **规范性 (Normalization)**：$P(\Omega) = 1$。
3.  **可加性 (Additivity)**：对于一列**互斥**的事件 $A_1, A_2, ...$，有：
    $$
    P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)
    $$

### 2.1.4 概率的基本性质

由上述三条公理可推导出以下常用性质：

-   $P(\emptyset) = 0$
-   **有限可加性 (Finite Additivity)**：若 $A_1, ..., A_n$ 互斥，则 $P(\bigcup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$。
-   **补集法则 (Complement Rule)**：$P(\bar{A}) = 1 - P(A)$。
-   **单调性 (Monotonicity)**：若 $A \subset B$，则 $P(A) \le P(B)$ 且 $P(B-A) = P(B) - P(A)$。
-   **加法公式 (Addition Law)**：
    $$
    P(A \cup B) = P(A) + P(B) - P(A \cap B)
    $$
-   **<u>容斥原理 (Inclusion-Exclusion Principle)</u>**：加法公式对多个事件的推广。
    $$
    P\left(\bigcup_{i=1}^n A_i\right) = \sum_{i=1}^n P(A_i) - \sum_{1 \le i < j \le n} P(A_i A_j) + \sum_{1 \le i < j < k \le n} P(A_i A_j A_k) - \dots + (-1)^{n-1} P(A_1 \dots A_n)
    $$

## 1.2 概率的计算

### 1.2.1 古典概型 (Classical Model of Probability)

这是最基础的概率模型，适用于满足以下两个条件的随机试验：
1.  样本空间 $\Omega$ 只包含**有限个**样本点。
2.  每个样本点发生的可能性**相同（等可能性）**。

在此模型下，事件 $A$ 的概率计算公式为：
$$
\mathbf{P(A) = \frac{事件A包含的样本点数}{样本空间的总样本点数}}
$$
因此，古典概型的计算核心在于**计数**。

-   **计数方法**：
    -   **加法原理 (Addition Principle)**：完成一件事有 $n$ 类方法，各类方法数分别为 $m_1, m_2, ..., m_n$，则总方法数为 $N = \sum m_i$。
    -   **乘法原理 (Multiplication Principle)**：完成一件事有 $n$ 个步骤，各步骤方法数分别为 $m_1, m_2, ..., m_n$，则总方法数为 $N = \prod m_i$。
    -   **排列 (Permutation)**：从 $n$ 个不同元素中取出 $k$ 个进行排序，其排列数为：
        $$
        A_n^k = n(n-1)...(n-k+1) = \frac{n!}{(n-k)!}
        $$
    -   **组合 (Combination)**：从 $n$ 个不同元素中取出 $k$ 个（不考虑顺序），其组合数为：
        $$
        C_n^k = \binom{n}{k} = \frac{n!}{k!(n-k)!}
        $$

> **用例：生日问题 (Birthday Problem)**
>
> **问题**：一个 $n$ 人的班级中，至少有两人 生日相同的概率是多少？（不考虑闰年）
> **分析**：这是一个典型的古典概型问题，关键在于正确地计数。
>
> 1.  **样本空间 $\Omega$**：每个人的生日可以是 365 天中的任意一天，所以 $n$ 个人的生日序列总数为 $365^n$。
> 2.  **事件 $A$**：“至少有两人 生日相同”。直接计算 $A$ 比较复杂，我们考虑其对立事件 $\bar{A}$：“所有人的生日都不同”。
> 3.  **计算 $\bar{A}$**：第一个人的生日有 365 种选择，第二个有 364 种，...，第 $n$ 个人有 $(365-n+1)$ 种。根据乘法原理，$\bar{A}$ 包含的样本点数为 $A_{365}^n = 365 \times 364 \times \dots \times (365-n+1)$。
> 4.  **计算概率**：
>     $$
>     P(A) = 1 - P(\bar{A}) = 1 - \frac{A_{365}^n}{365^n}
>     $$
> **结论**：当 $n=23$ 时，概率就超过了 $50\%$；当 $n=50$ 时，概率高达 $97\%$。这个结果通常与直觉相悖，说明了概率计算的重要性。

### 1.2.2 几何概型 (Geometric Model of Probability)

几何概型是古典概型的扩展，适用于样本点有**无穷多个**且**等可能**分布在一个几何区域（如线段、面积、体积）的情况。

事件 $A$ 的概率计算公式为：
$$
\mathbf{P(A) = \frac{构成事件A的区域测度(长度/面积/体积)}{样本空间的总测度(长度/面积/体积)}}
$$

> **循循善诱：贝特朗悖论 (Bertrand's Paradox)**
>
> **问题**：在一个圆内随机画一条弦，其长度大于该圆内接等边三角形边长的概率是多少？
>
> 这个问题之所以成为“悖论”，是因为“随机画一条弦”的定义不明确，导致了不同的样本空间和不同的答案。
>
> 1.  **方法一：随机中点法**
>     -   **随机方式**：在圆的一条半径上随机取一点作为弦的中点。
>     -   **样本空间**：半径上的所有点。
>     -   **分析**：只有当中点落在靠近圆心的半段半径上时，弦长才符合要求。
>     -   **概率**：$P = \frac{\text{内半段半径长度}}{\text{总半径长度}} = \frac{1}{2}$。
>
> 2.  **方法二：随机端点法**
>     -   **随机方式**：固定弦的一个端点，在圆周上随机取另一个端点。
>     -   **样本空间**：圆周上的所有点，对应 $180^\circ$ 的范围。
>     -   **分析**：只有当另一端点落在特定 $60^\circ$ 的弧上时，弦长才符合要求。
>     -   **概率**：$P = \frac{60^\circ}{180^\circ} = \frac{1}{3}$。
>
> 3.  **方法三：随机中点位置法**
>     -   **随机方式**：在圆内随机取一点作为弦的中点。
>     -   **样本空间**：整个圆的面积。
>     -   **分析**：只有当中点落在半径为原圆一半的同心圆内时，弦长才符合要求。
>     -   **概率**：$P = \frac{\text{小圆面积}}{\text{大圆面积}} = \frac{\pi (r/2)^2}{\pi r^2} = \frac{1}{4}$。
>
> **<u>结论</u>**：三种方法都正确，但它们基于对“随机”的不同理解，从而定义了不同的样本空间。**<u>在计算概率时，首要任务是清晰、无歧义地定义样本空间。</u>**

## 1.3 条件概率与独立性

### 1.3.1 条件概率 (Conditional Probability)

当已知某个事件 $B$ 发生后，事件 $A$ 发生的概率，称为在 $B$ 发生的条件下 $A$ 的**条件概率**，记为 $P(A|B)$。

-   **定义式**：
    $$
    P(A|B) = \frac{P(AB)}{P(B)}, \quad (\text{其中 } P(B) > 0)
    $$
-   **直观理解**：事件 $B$ 的发生提供了新的信息，使得样本空间从 $\Omega$ “缩减”到了 $B$。我们关心的就是在这个新样本空间 $B$ 中，$A$ 所占的比例。

### 1.3.2 重要定律

-   **乘法法则 (Multiplication Law)**：由条件概率定义直接变形得到，用于计算两事件同时发生的概率。
    $$
    P(AB) = P(A|B)P(B) = P(B|A)P(A)
    $$
    -   **链式法则 (Chain Rule)**：乘法法则的推广形式。
        $$
        P(A_1 A_2 \dots A_n) = P(A_1) P(A_2|A_1) P(A_3|A_1 A_2) \dots P(A_n|A_1 \dots A_{n-1})
        $$

-   **全概率公式 (Law of Total Probability)**：用于计算一个复杂事件的概率，其核心思想是“化整为零，分类讨论”。
    若事件 $B_1, B_2, \dots, B_n$ 构成样本空间 $\Omega$ 的一个**划分 (Partition)**（即它们互斥且并集为 $\Omega$），则对任意事件 $A$：
    $$
    \mathbf{P(A) = \sum_{i=1}^n P(A|B_i)P(B_i)}
    $$
    -   **作用**：**由因索果**。已知各种“原因” $B_i$ 的概率和在各原因下“结果” $A$ 发生的条件概率，求结果 $A$ 发生的总概率。

-   **<u>贝叶斯定理 (Bayes' Theorem)</u>**：全概率公式的“逆过程”。
    在全概率公式的条件下，若 $P(A) > 0$，则：
    $$
    \mathbf{P(B_k|A) = \frac{P(A|B_k)P(B_k)}{P(A)} = \frac{P(A|B_k)P(B_k)}{\sum_{i=1}^n P(A|B_i)P(B_i)}}
    $$
    -   **作用**：**由果溯因**。当观察到“结果” $A$ 已经发生时，反过来推断它是由哪个“原因” $B_k$ 导致的概率。
    -   **术语**：
        -   $P(B_k)$：**先验概率 (Prior Probability)**，即在获得新信息（$A$ 发生）前，对 $B_k$ 的判断。
        -   $P(B_k|A)$：**后验概率 (Posterior Probability)**，即在获得新信息后，对 $B_k$ 的修正判断。

> **循循善诱：贝叶斯定理的应用（罕见病检测）**
>
> **问题**：假设一种罕见病在人群中的发病率为 $0.01\%$（万分之一）。一种检测手段的准确率为：如果患病，有 $99\%$ 的概率检测为阳性（真阳性）；如果不患病，有 $1\%$ 的概率检测为阳性（假阳性）。现在一个人被检测为阳性，他真正患病的概率是多少？
>
> **分析**：
> 1.  **定义事件**：
>     -   $D$：此人患有该疾病。
>     -   $\bar{D}$：此人未患该疾病。
>     -   $T$：检测结果为阳性。
> 2.  **已知信息 (先验概率和条件概率)**：
>     -   $P(D) = 0.0001$ (先验概率)
>     -   $P(\bar{D}) = 1 - P(D) = 0.9999$
>     -   $P(T|D) = 0.99$ (真阳性率)
>     -   $P(T|\bar{D}) = 0.01$ (假阳性率)
> 3.  **求解目标**：$P(D|T)$ (后验概率)
> 4.  **应用贝叶斯定理**：
>     -   首先，用全概率公式计算 $P(T)$：
>         $$
>         P(T) = P(T|D)P(D) + P(T|\bar{D})P(\bar{D}) = (0.99 \times 0.0001) + (0.01 \times 0.9999) \approx 0.000099 + 0.009999 \approx 0.010098
>         $$
>     -   然后，计算后验概率 $P(D|T)$：
>         $$
>         P(D|T) = \frac{P(T|D)P(D)}{P(T)} = \frac{0.99 \times 0.0001}{0.010098} \approx 0.0098
>         $$
> **结论**：即使检测结果为阳性，此人真正患病的概率也**只有约 $0.98\%$**！这个与直觉严重不符的结果，是因为疾病的**先验概率极低**，导致了大量的假阳性淹没了真阳性。贝叶斯定理帮助我们进行这种理性的、反直觉的推理。

### 2.3.3 事件的独立性

-   **独立性 (Independence)**：如果事件 $B$ 的发生不影响事件 $A$ 发生的概率，则称 $A$ 与 $B$ 独立。
    -   **直观定义**：$P(A|B) = P(A)$
    -   **等价的计算定义**：
        $$
        \mathbf{P(AB) = P(A)P(B)}
        $$

-   **易混淆概念对比：独立 vs. 互斥**
	  
    | 特征 | **独立 (Independent)** | **互斥 (Mutually Exclusive)** |
    | :--- | :--- | :--- |
    | **定义** | $P(AB) = P(A)P(B)$ | $AB = \emptyset \implies P(AB) = 0$ |
    | **关系** | 概率关系，描述信息上的不相关。 | 集合关系，描述事件不能同时发生。 |
    | **联系** | 对于两个概率都大于 0 的事件 $A$ 和 $B$：**<u>如果它们互斥，那么它们一定不独立</u>**。因为 $P(A\vert B) = 0 \neq P(A)$，即 $B$ 的发生使得 $A$ 发生的概率变为 0，信息影响巨大。 |

-   **多个事件的相互独立 (Mutual Independence)**：
    对于 $n$ 个事件 $A_1, \dots, A_n$，它们相互独立**<u>当且仅当</u>**对于任意子集 $\{A_{i_1}, \dots, A_{i_k}\}$，都有：
    $$
    P(A_{i_1} \dots A_{i_k}) = P(A_{i_1}) \dots P(A_{i_k})
    $$
    **注意**：**两两独立 (Pairwise Independence) 并不能推出相互独立**。

-   **条件独立性 (Conditional Independence)**：
    事件 $A_1, \dots, A_n$ 在给定事件 $B$ 的条件下是条件独立的，如果：
    $$
    P(A_1 \dots A_n | B) = P(A_1|B) P(A_2|B) \dots P(A_n|B)
    $$
    这个概念是许多现代机器学习模型（如朴素贝叶斯分类器）的理论基石，它通过假设特征在给定类别下是独立的，从而大大简化了计算。